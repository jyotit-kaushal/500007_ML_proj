{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17184, 5002)\n"
     ]
    }
   ],
   "source": [
    "filename = '../Training and Testing sets/train_tfidf_features.csv'\n",
    "train_features = pd.read_csv (filename, header=0)\n",
    "\n",
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  label    0    1    2    3    4  ...  4993  4994  4995  4996  4997  4998  4999\n",
      "0   1      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "1   2      0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "2   3      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "3   4      0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "4   5      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "5   6      0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "6   7      0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "7   8      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "8   9      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "9  10      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "\n",
      "[10 rows x 5002 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label    0    1    2    3    4  ...  4994  4995  4996  4997  4998  4999\n",
      "id                                  ...                                    \n",
      "1       1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "2       0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "3       1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "4       0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "5       1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "6       0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "7       0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "8       1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "9       1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "10      1  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "\n",
      "[10 rows x 5001 columns]\n"
     ]
    }
   ],
   "source": [
    "train_features.set_index('id', inplace=True, drop=True)\n",
    "print(train_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        ## which feature to use for splitting dataset\n",
    "        self.feature_index = feature_index\n",
    "        ## threshold value to split dataset\n",
    "        self.threshold = threshold\n",
    "        ## root node of the left subtree\n",
    "        self.left = left\n",
    "        ## root node of the left subtree\n",
    "        self.right = right\n",
    "        ## information gained from the this node\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed up the processing\n",
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # initialize the root of the tree \n",
    "        self.root = None\n",
    "        \n",
    "        # stopping conditions\n",
    "        ## if number of data samples in dataset to split using build_tree cannot be \n",
    "        ## smaller than min_samples_split (default to 2 points)\n",
    "        self.min_samples_split = min_samples_split\n",
    "        ## building a tree with ever increasing depth as it recurses maximum depth of \n",
    "        ## a branch cannot exceed max_depth(default 3 levels if root node is included)\n",
    "\n",
    "        # TODO \n",
    "        # As the dataset is big, it is prone to overfitting, we are unlikely to \n",
    "        # split the dataset into less than min_samples_split. Hence, it is likely we should\n",
    "        # instead determin the max_depth of the tree. \n",
    "        # Hence making max_depth our HYPERPARAMETER\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, dataset, d=0):\n",
    "        ''' recursive function to build the tree ''' \n",
    "        \n",
    "        X, Y = dataset[:,:-1], dataset[:,-1]\n",
    "        num_samples, num_features = np.shape(X)\n",
    "        \n",
    "        # only split if sample is large enough or max depth is not exceeded\n",
    "        if num_samples>=self.min_samples_split and d<=self.max_depth:\n",
    "            # find the best split\n",
    "            # returns a dictionary that characterises the best_split (with the most information gain) \n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "            # check if information gain is positive\n",
    "            if best_split[\"info_gain\"]>0:\n",
    "                # recursion: build_tree on the left, left_subtree is the root node of the left subtree\n",
    "                ## left_subtree = self.build_tree(best_split[\"dataset_left\"], d+1)\n",
    "                p = Process(target=self.build_tree, args=(best_split[\"dataset_left\"],d+1))\n",
    "                # recursion: build_tree on the right, right_subtree is the root node of right subtree\n",
    "                ## right_subtree = self.build_tree(best_split[\"dataset_right\"], d+1)\n",
    "                p = Process(target=self.build_tree, args=(best_split[\"dataset_right\"],d+1))\n",
    "                # return current decision node (with inputs left node, right node and node attributes)\n",
    "                # not a leaf so no value\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        # only leaf node has value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        # max_info_gain from any decision threshold boundary to be negative infinity (monotonically increasing)  \n",
    "        max_info_gain = -float(\"inf\")\n",
    "        \n",
    "        # For each feature i\n",
    "        for feature_index in range(num_features):\n",
    "            # Finding unique values for a particular feature\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # For every unique value/possible threshold in feature i, calculate the information gain\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0:\n",
    "                    y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    # compute information gain\n",
    "                    curr_info_gain = self.information_gain(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_info_gain>max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])\n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    # TODO\n",
    "    # Alternatively, entropy can be used instead of gini_index but because entropy is more computationally\n",
    "    # intensive so it was not used. Loss function can be considered a HYPERPARAMETER. Entropy criterion\n",
    "    # only performs marginally better but might be worth exploring\n",
    "    def information_gain(self, parent, l_child, r_child):\n",
    "        ''' function to compute information gain '''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))\n",
    "       \n",
    "        return gain\n",
    "    \n",
    "    def gini_index(self, y):\n",
    "        ''' function to compute gini index '''\n",
    "\n",
    "        # Formula of gini index is 1 - summation((probability of class i)**2)\n",
    "        p1_sq = (len(y[y == 1]) / len(y))**2\n",
    "        p0_sq = (len(y[y == 0]) / len(y))**2\n",
    "        gini = 1- p1_sq - p0_sq\n",
    "\n",
    "        return gini\n",
    "        \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute leaf node '''\n",
    "        \n",
    "        # value of leaf value == most commonly found class in the leaf within data \n",
    "        Y = list(Y)\n",
    "        return max(Y, key=Y.count)\n",
    "    \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"Feature: \"+str(tree.feature_index), \"<=\", tree.threshold, \",info_gain: \", tree.info_gain)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions\n",
    "    \n",
    "    def make_prediction(self, x, node):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        # If leaf node\n",
    "        if node.value!=None: \n",
    "            return node.value\n",
    "        # If not leaf node, need to determine whether to continue on the left or right node\n",
    "        feature_val = x[node.feature_index]\n",
    "        if feature_val<=node.threshold:\n",
    "            return self.make_prediction(x, node.left)\n",
    "        else:\n",
    "            return self.make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from df_helper.reduction_helper import PCA_reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1    2    3    4    5    6  ...  4993  4994  4995  4996  4997  4998  4999\n",
      "id                                     ...                                          \n",
      "1   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "2   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "3   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "4   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "5   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "6   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "7   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "8   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "9   0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "10  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0\n",
      "\n",
      "[10 rows x 5000 columns]\n",
      "         0         1         2   ...        47        48        49\n",
      "0 -0.083197 -0.016050 -0.010587  ...  0.038068 -0.018019  0.010699\n",
      "1 -0.068420 -0.043651 -0.018440  ... -0.013614 -0.034725 -0.012694\n",
      "2 -0.080171 -0.044643 -0.015336  ...  0.072818  0.001175 -0.127964\n",
      "3  0.028600 -0.040405  0.002782  ... -0.021785 -0.013191  0.020190\n",
      "4  0.255054 -0.113418 -0.019241  ... -0.008976 -0.003493 -0.000353\n",
      "5 -0.066720  0.042433  0.201004  ... -0.017364 -0.017860  0.035051\n",
      "6 -0.020097  0.090491  0.039992  ... -0.037783 -0.020138  0.027464\n",
      "7 -0.071623  0.007542 -0.008370  ... -0.003795  0.003602  0.006936\n",
      "8 -0.030578  0.122266 -0.054873  ... -0.058499  0.039395 -0.034038\n",
      "9  0.302060  0.479296 -0.182746  ... -0.004719  0.006359 -0.007533\n",
      "\n",
      "[10 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate a 100 feature feature set using PCA reduction\n",
    "print(train_features.iloc[:, 1:].head(10))\n",
    "X = PCA_reduce(train_features.iloc[:, 1:],50)\n",
    "print(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_features.iloc[:, 10:20].values\n",
    "Y = train_features.iloc[:, 0].values.reshape(-1,1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=0'>1</a>\u001b[0m classifier \u001b[39m=\u001b[39m DecisionTreeClassifier(min_samples_split\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=1'>2</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train,Y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=2'>3</a>\u001b[0m classifier\u001b[39m.\u001b[39mprint_tree()\n",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=137'>138</a>\u001b[0m \u001b[39m''' function to train the tree '''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=139'>140</a>\u001b[0m dataset \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((X, Y), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=140'>141</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_tree(dataset)\n",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36mDecisionTreeClassifier.build_tree\u001b[0;34m(self, dataset, curr_depth)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=27'>28</a>\u001b[0m \u001b[39m# only split if sample is large enough or max depth is not exceeded\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m num_samples\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_samples_split \u001b[39mand\u001b[39;00m curr_depth\u001b[39m<\u001b[39m\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_depth:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=29'>30</a>\u001b[0m     \u001b[39m# find the best split\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=30'>31</a>\u001b[0m     \u001b[39m# returns a dictionary that characterises the best_split (with the most information gain) \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=31'>32</a>\u001b[0m     best_split \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_best_split(dataset, num_samples, num_features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=32'>33</a>\u001b[0m     \u001b[39m# check if information gain is positive\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m best_split[\u001b[39m\"\u001b[39m\u001b[39minfo_gain\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=34'>35</a>\u001b[0m         \u001b[39m# recursion: build_tree on the left, left_subtree is the root node of the left subtree\u001b[39;00m\n",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36mDecisionTreeClassifier.get_best_split\u001b[0;34m(self, dataset, num_samples, num_features)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=68'>69</a>\u001b[0m y, left_y, right_y \u001b[39m=\u001b[39m dataset[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dataset_left[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dataset_right[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=69'>70</a>\u001b[0m \u001b[39m# compute information gain\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=70'>71</a>\u001b[0m curr_info_gain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minformation_gain(y, left_y, right_y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=71'>72</a>\u001b[0m \u001b[39m# update the best split if needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m curr_info_gain\u001b[39m>\u001b[39mmax_info_gain:\n",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36mDecisionTreeClassifier.information_gain\u001b[0;34m(self, parent, l_child, r_child)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=97'>98</a>\u001b[0m weight_l \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(l_child) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(parent)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=98'>99</a>\u001b[0m weight_r \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(r_child) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(parent)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=99'>100</a>\u001b[0m gain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgini_index(parent) \u001b[39m-\u001b[39m (weight_l\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgini_index(l_child) \u001b[39m+\u001b[39m weight_r\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgini_index(r_child))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m gain\n",
      "\u001b[1;32m/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine Learning | 50.007/500007_ML_proj/Decision Tree.ipynb Cell 17\u001b[0m in \u001b[0;36mDecisionTreeClassifier.gini_index\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=104'>105</a>\u001b[0m \u001b[39m''' function to compute gini index '''\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=106'>107</a>\u001b[0m \u001b[39m# Formula of gini index is 1 - summation((probability of class i)**2)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=107'>108</a>\u001b[0m p1_sq \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39;49m(y[y \u001b[39m==\u001b[39;49m \u001b[39m1\u001b[39;49m]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=108'>109</a>\u001b[0m p0_sq \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(y[y \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/waishun/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/SUTD_Y2/Term_5/Machine%20Learning%20%7C%2050.007/500007_ML_proj/Decision%20Tree.ipynb#ch0000013?line=109'>110</a>\u001b[0m gini \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m-\u001b[39m p1_sq \u001b[39m-\u001b[39m p0_sq\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=4)\n",
    "classifier.fit(X_train,Y_train)\n",
    "classifier.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6045970322956067"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = classifier.predict(X_test) \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Do cross validation so that machine learning model is performs more consistently. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f9328efe3468e6c370cdfed98702d3986faf748314d5bcec59da615d65baa7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
