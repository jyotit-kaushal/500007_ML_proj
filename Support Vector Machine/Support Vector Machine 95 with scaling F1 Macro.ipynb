{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine Learning Model (95% variance, with scaling - F1 Macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for multi-dimensional array operations\n",
    "import pandas as pd # for reading data from .csv files\n",
    "from sklearn.svm import SVC # for support vector machine model\n",
    "from sklearn.decomposition import PCA # for principle component analysis (dimensionality reduction)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold # for getting the best hyper parameters\n",
    "from sklearn.preprocessing import MinMaxScaler # for scaling of data before PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the training set and testing set to variables for easy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('../train_tfidf_features.csv') # import the training set\n",
    "test_set = pd.read_csv('../test_tfidf_features.csv') # import the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis for train_set (95% variance, with scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_label = train_set.loc[:, [\"label\"]]\n",
    "features_names = [str(i) for i in range(0, 5000)]\n",
    "train_set_features = train_set.loc[:, features_names] # train_set_features will not contain the label and id columns\n",
    "\n",
    "# scale the dataset before PCA\n",
    "scaler = MinMaxScaler()\n",
    "train_set_rescaled = scaler.fit_transform(train_set_features)\n",
    "\n",
    "# perform PCA\n",
    "pca = PCA(n_components = 0.95)\n",
    "train_set_reduced = pca.fit_transform(train_set_rescaled)\n",
    "train_set_reduced = pd.DataFrame(data = train_set_reduced)\n",
    "train_set_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set_reduced\n",
    "y = train_set_label\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) # Train a SVC model using different kernel\n",
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis for test_set (95% variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = [str(i) for i in range(0, 5000)]\n",
    "test_set_features = test_set.loc[:, features_names] # test_set_features will not contain the label and id columns\n",
    "\n",
    "# scale the dataset before PCA\n",
    "test_set_rescaled = scaler.transform(test_set_features)\n",
    "\n",
    "# perform PCA\n",
    "test_set_reduced = pca.transform(test_set_rescaled) # use the pca from the train_set?\n",
    "test_set_features = pd.DataFrame(data = test_set_reduced)\n",
    "test_set_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the hyper-parameters and training the model based on the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper_parameters = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['linear', 'poly', 'sigmoid', 'rbf']} # initialise the hyper-parameters\n",
    "hyper_parameters = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']} # initialise the hyper-parameters\n",
    "kfold = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 0) # for 3-fold cross validation\n",
    "# grid = GridSearchCV(SVC(), hyper_parameters, refit = True, verbose = 2) # cretae a GridSearchCV object to git to the taining data\n",
    "grid = GridSearchCV(SVC(), param_grid = hyper_parameters, scoring = 'f1_macro', refit = 'f1_macro', n_jobs = 1 , cv = kfold, verbose = 2)\n",
    "grid.fit(X_train, np.ravel(y_train)) # training the model using the best hyper-parameters\n",
    "print(grid.best_params_) # gets the best hyper-parameters for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the labels for the test dataset based on the model with the best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = grid.predict(test_set_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted = svc_model.predict(test_set_features)\n",
    "y_predicted = pd.DataFrame(y_predicted, columns = ['label']) # convert y_predicted from nparray to pandas dataframe\n",
    "y_predicted.insert(loc = 0, column = 'id', value = [i for i in range(17185, 17185 + 4296)]) # insert a column of the ids, starting from 17185\n",
    "y_predicted.to_csv('skynet_submission_95_with_scaling_f1macro.csv', index = False) # output the predicted labels to ./skynet_submission_95_with_scaling_f1macro.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b2b11d8b9d885082e3c8ebab2eb585e28a9debb2f8189a1d20e3fb095fdc50f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
